{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer,Dense,Multiply,Input,Activation\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create customized neuron and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "We create a single nuron NN with a customized activation function.\n",
    "\n",
    "\n",
    "Inputs are given by\n",
    "- units:int = number of neurons in the NN (1 -> single neuron)\n",
    "- nvar:int = number of input variables (number of columns in dataset, i.e. number features) \n",
    "- npar:int = number of numerical parameters appearing in program (number of float numbers used in program)\n",
    "- minval:float = lower bound in float range\n",
    "- maxval:float = upper bound in float range\n",
    "\n",
    "\n",
    "Build initializes the neuron weights. \n",
    "Weights are trainable parameters to be used as float values in symbolic regression after training\n",
    "- weights appear as a numpy array of shape (units,npar) \n",
    "- weights are initially uniformly sampled in the range [minval,maxval]\n",
    "- weights are set as trainable\n",
    "\n",
    "\n",
    "Call computes the neuron output:\n",
    "- activation function is set to program_function that is defined outside the class  \n",
    "- program_function(inputs, self.w, self.nvar, self.npar) contains the skeleton of the \n",
    "  binary tree created by symbolic regression, i.e. the functional dependence but \n",
    "  no predefined parameters. (See program_function)\n",
    "'''\n",
    "\n",
    "class ProgramNN(Layer):\n",
    "\n",
    "    def __init__(self, units:int, nvar:int ,npar:int, minval:float, maxval:float):\n",
    "#         '''Initializes the class and sets up the internal variables'''\n",
    "        super(ProgramNN, self).__init__()\n",
    "        self.units = units\n",
    "        self.npar=npar\n",
    "        self.nvar=nvar\n",
    "        self.minval=minval\n",
    "        self.maxval=maxval\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "#         '''Create the state of the layer (weights)'''\n",
    "        w_init = tf.random_uniform_initializer(minval=self.minval, maxval=self.maxval)\n",
    "        self.w= self.add_weight(name=\"w\",shape=(self.units,self.npar),dtype='float32',\n",
    "                                regularizer=None,\n",
    "                                initializer=w_init,trainable=True)\n",
    "        super().build(input_shape)\n",
    "   \n",
    "    def call(self, inputs):\n",
    "#         '''Defines the computation from inputs to outputs'''\n",
    "        return program_function(inputs, self.w, self.nvar, self.npar)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Previously, the symbolic regression created functions as:\n",
    "  \n",
    "  data <- dataset given by a pd.Dataset with coumns names ['x_0','x_1','x_2']\n",
    "  program.program = 'exp(-0.1 * x_0) + 1.2 * x_1 * x_2'\n",
    "  \n",
    "Now we need to create a function (program_function) based on the tree structure that takes as inputs:\n",
    "  \n",
    "  - X:tf.constant(data.to_numpy()) \n",
    "    dataset of features (dataset: [x[0],x[1],x[2],....]  x[i]=features[i]=i-th column)  \n",
    "    \n",
    "    !!!X is passed as argument in ProgramNN.call as input !!!\n",
    "    \n",
    "  - parameters:tf.constant([par[0],par[1],..])   \n",
    "    array of parameters to be used in the function\n",
    "    \n",
    "    !!! parameters is initialized as w in ProgramNN.build !!!\n",
    "    \n",
    "  - nvar:int\n",
    "    number of columns in dataset\n",
    "    \n",
    "  - npar:int \n",
    "    number of float number used in program\n",
    "    \n",
    "    !!! nvar, npar are set in the ProgramNN.init   !!!\n",
    "\n",
    "\n",
    "and does the following: \n",
    "\n",
    "- split the tensor X (dataset) in its columns  [x_0,x_1,x_2]-> [x_0], [x_1], [x_2]\n",
    "  so that we can use each variable column independently\n",
    "  \n",
    "- split the tensor parameters in single values [par_1,par_2]-> [par_1], [par_2]\n",
    "  so that we can use each parameter independently\n",
    "\n",
    "- defines the function (that was given by program.program = 'exp(-0.1 * x_0) + 1.2 * x_1 * x_2') as:\n",
    "\n",
    "  fun=tf.exp(par[0]*x_0)+par[1]* x_1 * x_2\n",
    "  \n",
    "   ######## \n",
    "   ##!!!!## np.fun -> tf.func   numpy fuctions need to be transformed in tf.functions\n",
    "   ##!!!!## pd.dataset -> tf.constant   dataset needs to be transformed in in tf.constant\n",
    "   ########\n",
    "\n",
    "- returns fun\n",
    "\n",
    "'''\n",
    "def program_function(X:tf.constant,\n",
    "                     parameters:tf.constant,\n",
    "                     nvar:int,\n",
    "                     npar:int):\n",
    "    \n",
    "    x=tf.split(X,nvar,1)\n",
    "    par=tf.split(parameters,npar,1)\n",
    "    fun=x[0]**2+tf.exp(par[0]*x[2])-tf.abs(par[1]*x[3])\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simuliamo un potenziale target (indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simulating a target that has the same functional form as our program from the symbolic regression.\n",
    "Of course, we need to fix the parameters to well defined values\n",
    "inputs are given by \n",
    "X:tf.constant() = dataset transformed to tensorflow constant\n",
    "nvar:int = number of columns in dataset (number of variables)\n",
    "'''\n",
    "def real_data(X:tf.constant,\n",
    "              nvar:int):\n",
    "    x=tf.split(X,nvar,1)\n",
    "    fun=x[0]**2+tf.exp(1.4*x[2])-tf.abs(0.8*x[3])\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Customised one step training: \n",
    "compute loss gradients\n",
    "update model trainable weights using optimizer\n",
    "return new loss value\n",
    "'''\n",
    "def one_training_step(model:tf.keras.Model, \n",
    "                      x:tf.constant, \n",
    "                      y_true:tf.constant, \n",
    "                      weights:tf.constant, \n",
    "                      opt:tf.keras.optimizers, \n",
    "                      loss_function:tf.keras.losses):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "            y_pred=model(x, training=True)\n",
    "\n",
    "            loss = loss_fun(y_true,y_pred,sample_weight=weights)\n",
    "            \n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>target</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.221430</td>\n",
       "      <td>1.085235</td>\n",
       "      <td>1.329246</td>\n",
       "      <td>-0.538409</td>\n",
       "      <td>6.048107</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.018574</td>\n",
       "      <td>-0.617236</td>\n",
       "      <td>-0.138118</td>\n",
       "      <td>0.365174</td>\n",
       "      <td>1.569535</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.979786</td>\n",
       "      <td>0.143209</td>\n",
       "      <td>0.143652</td>\n",
       "      <td>1.312779</td>\n",
       "      <td>1.132519</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.364351</td>\n",
       "      <td>-0.926683</td>\n",
       "      <td>-0.114177</td>\n",
       "      <td>1.026624</td>\n",
       "      <td>1.892427</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.267300</td>\n",
       "      <td>-0.064395</td>\n",
       "      <td>0.527368</td>\n",
       "      <td>-2.955694</td>\n",
       "      <td>-0.200700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_0       x_1       x_2       x_3    target  weights\n",
       "0 -0.221430  1.085235  1.329246 -0.538409  6.048107      1.0\n",
       "1  1.018574 -0.617236 -0.138118  0.365174  1.569535      1.0\n",
       "2 -0.979786  0.143209  0.143652  1.312779  1.132519      1.0\n",
       "3  1.364351 -0.926683 -0.114177  1.026624  1.892427      1.0\n",
       "4  0.267300 -0.064395  0.527368 -2.955694 -0.200700      1.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "CREATING A FAKE DATASET THAT LOOKS LIKE OURS\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['x_0']=np.random.normal(size=(1000,))\n",
    "df['x_1']=np.random.normal(size=(1000,))\n",
    "df['x_2']=np.random.normal(size=(1000,))\n",
    "df['x_3']=np.random.normal(size=(1000,))\n",
    "dataset_to_tensor=tf.constant(df.to_numpy(),dtype=tf.float32)\n",
    "df['target']=real_data(dataset_to_tensor,len(df.columns)).numpy()\n",
    "df['weights']=tf.ones((df.shape[0],1)).numpy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a function to define how to split the dataset\n",
    "(Noi ce l'abbiamo già credo, comunque non fa nient'altro che prendermi le colonne che mi pare\n",
    " per dividere le x dal target e dai pesi)\n",
    "'''\n",
    "def to_split_map(split_range):\n",
    "    def split_window(df):  \n",
    "        features = tf.slice(df,[0,split_range[0][0]],[-1,split_range[0][1]])\n",
    "        labels = tf.slice(df,[0,split_range[1][0]],[-1,split_range[1][1]])\n",
    "        weights = tf.slice(df,[0,split_range[2][0]],[-1,split_range[2][1]])\n",
    "        return features, labels, weights\n",
    "    return split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 19:09:19.168318: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real parameters: [1.4, (+/-) 0.8]\n",
      "Estimate parameters: [ 1.4013735  -0.79195994]\n",
      "Final Loss: 2.068160057067871\n",
      "Training time: 2.1561927795410156\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training della rete, calcolo dei parametri e della fitness\n",
    "\n",
    "\n",
    "N.B. In realtà potremmo anche decidere di fare sta cosa su un subset del dataset totale\n",
    "'''\n",
    "\n",
    "t=time.time()\n",
    "nvar=4         #number of features\n",
    "npar=2         #number of parameters in program\n",
    "minval=-1.      #minimum value to initialize the parameters\n",
    "maxval=1.      #maximum value to initialize the parameters\n",
    "epochs=1       #number of epochs\n",
    "batch_size=16   #number of data points per batch\n",
    "\n",
    "#CREATE TF VARIABLES FROM DATASET (to be changed, can be easier)\n",
    "split_range=[[0,4],[4,1],[5,1]]                   #columns related to features, target, weights [start,n.columns]\n",
    "split_map=to_split_map(split_range)               #split map\n",
    "tfdf=tf.constant(df.to_numpy(),dtype=tf.float32)  #pandas Dataset into tf.constant\n",
    "X,y_true,weights=split_map(tfdf)                  #input, target, weights from map\n",
    "\n",
    "#CREATE NEURON\n",
    "inputs = Input(shape=[nvar], name=\"Input\")\n",
    "program = ProgramNN(units=1, nvar=nvar,npar=npar, minval=minval, maxval=maxval)(inputs)\n",
    "model = Model(inputs=inputs, outputs=program)\n",
    "\n",
    "#DEFINE LOSS FUNCTION AND OPTIMIZER\n",
    "loss_mse = tf.keras.losses.MeanSquaredError()     #weights are passed to model.fit as sample_weight\n",
    "opt= tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "\n",
    "#COMPILE THE MODEL\n",
    "model.compile(loss=loss_mse, optimizer=opt,run_eagerly=False)\n",
    "\n",
    "#TRAIN THE MODEL\n",
    "history = model.fit(X, y_true, sample_weight=weights, batch_size=1, epochs=epochs,verbose=0)\n",
    "\n",
    "#COMPUTE FINAL FITNESS AND PARAMETERS\n",
    "fitness=(history.history['loss'][-1])\n",
    "final_parameters=model.get_weights()[0][0]\n",
    "print(f'Real parameters: [1.4, (+/-) 0.8]')\n",
    "print(f'Estimate parameters: {final_parameters}')\n",
    "print(f'Final Loss: {fitness}')\n",
    "print(f'Training time: {time.time()-t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
