{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../')  # to include the SR code without installing in the environment\n",
    "\n",
    "from symbolic_regression.SymbolicRegressor import SymbolicRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The operations\n",
    "\n",
    "Here we define the list of allowed operations. In this project we implemented most of the arithmetic operations we expect to need in a normal use. Please have a look at the file in `symbolic_regression/operators.py` to see how we define them and to define your own operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symbolic_regression.operators import *\n",
    "\n",
    "operations = [\n",
    "    OPERATOR_ADD,\n",
    "    OPERATOR_SUB,\n",
    "    OPERATOR_MUL,\n",
    "    OPERATOR_DIV,\n",
    "    # OPERATOR_ABS,\n",
    "    # OPERATOR_MOD,\n",
    "    # OPERATOR_NEG,\n",
    "    # OPERATOR_INV,\n",
    "    OPERATOR_LOG,\n",
    "    OPERATOR_EXP,\n",
    "    OPERATOR_POW,\n",
    "    OPERATOR_SQRT,\n",
    "    OPERATOR_MAX,\n",
    "    OPERATOR_MIN\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The example dataset: Body Fat Index\n",
    "\n",
    "This is the generation of a score to predict the Body Fat Intex of a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "def std_normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        mean_val = df[feature_name].mean()\n",
    "        std_val  = df[feature_name].std()\n",
    "        result[feature_name] = (df[feature_name] - mean_val) / std_val\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(f'./body_fat.csv')\n",
    "data=data.drop(41)  # Drop the outlier\n",
    "\n",
    "### Data engineering and normalization\n",
    "\n",
    "Weight_lb_to_kg=data['Weight']*0.453592\n",
    "Height_inches_to_m=data['Height']*0.0254\n",
    "BMI=Weight_lb_to_kg/(Height_inches_to_m**2)\n",
    "\n",
    "proxy=(BMI-BMI.mean())/BMI.std()\n",
    "\n",
    "data=std_normalize(data)\n",
    "\n",
    "bins=16\n",
    "\n",
    "features = list(data.columns)\n",
    "features.remove('BodyFat')\n",
    "features.remove('Density')\n",
    "target='BodyFat'\n",
    "\n",
    "from symbolic_regression.multiobjective.functions import get_cumulant_hist, create_regression_weights\n",
    "F_y1=get_cumulant_hist(data=data,target=target,bins=bins)\n",
    "data['w']=create_regression_weights(data=data,target=target,bins=bins)\n",
    "weights='w'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the base range for which to generate the constants in the individuals. Furthermore, we also define how to optimize those constants in order to make them converge to the best value they can have in their expression.\n",
    "\n",
    "We are using ADAM with the following configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_range = (0, 1)\n",
    "\n",
    "constants_optimization = True\n",
    "constants_optimization_method= 'ADAM'\n",
    "constants_optimization_conf = {\n",
    "    'task': 'regression:wmse', #or 'binary:logistic'\n",
    "    'learning_rate': 1e-4,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 50,\n",
    "    'verbose': 0,\n",
    "    'gradient_clip':False,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'epsilon': 1e-7    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This implements the error functions to be minimized.\n",
    "The arguments of this function must not change as the code expects them to be\n",
    "as appear here.\n",
    "\n",
    "Use a dict with this structure:\n",
    "    {\n",
    "        <name_of_the_error_function>: {\n",
    "            'func': the actual function to be evaluated,\n",
    "            'threshold': the value below which convergence happens # Optional\n",
    "        }\n",
    "    }\n",
    "'''\n",
    "\n",
    "\n",
    "def multi_obj(program, data):\n",
    "    from symbolic_regression.multiobjective.functions import (\n",
    "        wmse, kendall_correlation, wasserstein)\n",
    "\n",
    "    ''' Some examples are\n",
    "    \n",
    "    'not_constant': {\n",
    "        'func': not_constant(program=program, data=data, epsilon=.01)\n",
    "        }\n",
    "    'wmse': {\n",
    "        'func': wmse(program=program, data=data, target=target, weights=weights),\n",
    "        'convergence_threshold': .95\n",
    "        },\n",
    "    'bce': {\n",
    "        'func': binary_cross_entropy(\n",
    "            program=program, data=data, target='y', logistic=True,\n",
    "            constants_optimization=constants_optimization,\n",
    "            constants_optimization_method=constants_optimization_method,\n",
    "            constants_optimization_conf=constants_optimization_conf\n",
    "            )\n",
    "        },\n",
    "    'AUC': {\n",
    "        'func': auroc_bce(program=program, data=data, target='y', logistic=True),\n",
    "        'minimize': False\n",
    "    },\n",
    "    '''\n",
    "    return {\n",
    "        'wmse': {\n",
    "            'func': wmse(program=program, data=data, target=target, weights=weights,\n",
    "                         constants_optimization=constants_optimization,\n",
    "                         constants_optimization_method=constants_optimization_method,\n",
    "                         constants_optimization_conf=constants_optimization_conf),\n",
    "            'convergence_threshold': 0.001,\n",
    "            'minimize': True\n",
    "        },\n",
    "        'op': {  # The one_minus operator is used to maximize the correlation.\n",
    "            'func': kendall_correlation(program=program, data=data, target=target, one_minus=True),\n",
    "            'minimize': True\n",
    "        },\n",
    "        'wasserstein': {\n",
    "            'func': wasserstein(program=program, data=data, F_y=F_y1),\n",
    "            'minimize': True\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "''' Use this to modulate the relative frequency of genetic operations\n",
    "    E.g., crossover is chosen 2 times more frequently than mutation\n",
    "        {\n",
    "            'crossover': 2,\n",
    "            'mutation': 1,\n",
    "            # etc...\n",
    "        }\n",
    "'''\n",
    "genetic_operators_frequency = {\n",
    "    'crossover': 1,\n",
    "    'randomize': 1,\n",
    "    'mutation': 1,\n",
    "    'insert_node': 1,\n",
    "    'delete_node': 1,\n",
    "    'mutate_leaf': 1,\n",
    "    'mutate_operator': 1,\n",
    "    'recalibrate': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a population size of 100 individuals, the training process to be 20 generations long and the tournament size for the genetic operations to be 3.\n",
    "\n",
    "Setting the checkpoint file allows us to progressively save the population to recover the training in future training sessions or to share the population with other participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPULATION_SIZE = 300\n",
    "GENERATIONS = 200\n",
    "TOURNAMENT_SIZE = 3\n",
    "\n",
    "logging.info(f'Running with POPULATION_SIZE {POPULATION_SIZE}')\n",
    "logging.info(f'Running with GENERATIONS {GENERATIONS}')\n",
    "logging.info(f'Running with TOURNAMENT_SIZE {TOURNAMENT_SIZE}')\n",
    "\n",
    "\n",
    "sr = SymbolicRegressor(\n",
    "    checkpoint_file='./body_fat_checkpoint.save',\n",
    "    checkpoint_frequency=10,\n",
    "    const_range=const_range,\n",
    "    parsimony=.8,\n",
    "    parsimony_decay=.85,  # Expected depth = parsimony / (1-parsimony_decay)\n",
    "    population_size=POPULATION_SIZE,\n",
    "    tournament_size=TOURNAMENT_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sr.fit(\n",
    "    data=data,\n",
    "    features=features,\n",
    "    fitness_functions=multi_obj,\n",
    "    generations=GENERATIONS,\n",
    "    genetic_operators_frequency=genetic_operators_frequency,\n",
    "    operations=operations,\n",
    "    n_jobs=-1,\n",
    "    stop_at_convergence=False,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
